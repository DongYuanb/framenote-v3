"""å¤šæ¨¡æ€æœåŠ¡ - ç»Ÿä¸€çš„å¸§æå–ã€åµŒå…¥ç”Ÿæˆå’Œå»é‡æœåŠ¡"""
import os,base64,time,logging,tempfile,shutil,json,concurrent.futures,threading
from typing import List,Optional,Dict,Any,Tuple
from datetime import datetime
from pathlib import Path
import cohere,numpy as np
from services.ffmpeg_process import VideoProcessor
from PIL import Image
import faiss


class MultimodalService:
    """å¤šæ¨¡æ€æœåŠ¡ - ç»Ÿä¸€å¤„ç†å¸§æå–ã€åµŒå…¥ç”Ÿæˆå’Œå»é‡"""
    EMBED_MODEL="embed-v4.0"
    BATCH_SIZE=10
    API_DELAY=0.1
    IMG_FORMATS={'.png':'image/png','.jpg':'image/jpeg','.jpeg':'image/jpeg','.gif':'image/gif','.webp':'image/webp'}

    def __init__(self,cohere_api_key:str,ffmpeg_path:str="ffmpeg",similarity_threshold:float=0.9,
                 embedding_model:str=EMBED_MODEL,batch_size:int=BATCH_SIZE,frame_fps:float=0.2,
                 max_concurrent_segments:int=3,enable_text_alignment:bool=True,max_aligned_frames:int=3,
                 logger:Optional[logging.Logger]=None,task_id:Optional[str]=None):
        """åˆå§‹åŒ–å¤šæ¨¡æ€æœåŠ¡"""
        self.api_key=cohere_api_key
        self.sim_thresh=similarity_threshold
        self.embed_model=embedding_model
        self.batch_sz=batch_size
        self.fps=frame_fps
        self.max_workers=max_concurrent_segments
        self.enable_text_alignment=enable_text_alignment
        self.max_aligned_frames=max_aligned_frames
        self.client=cohere.ClientV2(api_key=cohere_api_key)
        self.video_proc=VideoProcessor(ffmpeg_path)
        self.logger=logger or logging.getLogger(__name__)
        self._lock=threading.Lock()
        # æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹çš„åµŒå…¥ç¼“å­˜ï¼Œé¿å…è·¨ç”¨æˆ·æ±¡æŸ“
        self.task_id=task_id or "default"
        self._emb_cache:Dict[str,np.ndarray]={}

    def _ahash(self,p:str,size:int=8)->int:
        img=Image.open(p).convert('L').resize((size,size))
        arr=np.array(img);m=arr.mean();bits=(arr>m).flatten();h=0
        for b in bits:h=(h<<1)|(1 if b else 0)
        return h

    def _prefilter_by_hash(self,paths:List[str])->List[str]:
        seen=set();out=[]
        for p in paths:
            try:
                h=self._ahash(p)
                if h in seen:continue
                seen.add(h);out.append(p)
            except Exception as e:
                self.logger.warning(f"Hash fail {p}: {e}");out.append(p)
        return out

    def _choose_fps(self,dur:float,target:int=10,fp_min:float=0.1,fp_max:float=1.0)->float:
        if dur<=0:return self.fps
        return max(fp_min,min(fp_max,target/dur))

    def extract_frames(self,video_path:str,start_time:float,end_time:float,fps:float=1.0,output_dir:Optional[str]=None)->List[str]:
        """ä»è§†é¢‘æŒ‡å®šæ—¶é—´èŒƒå›´æŠ½å–å¸§"""
        return self.video_proc.extract_frames(video_path,start_time,end_time,fps,output_dir)

    def _get_mime_type(self,path:str)->str:
        """æ£€æµ‹å›¾ç‰‡MIMEç±»å‹"""
        p=path.lower()
        for ext,mime in self.IMG_FORMATS.items():
            if p.endswith(ext):return mime
        return "image/jpeg"

    def _to_base64(self,path:str)->str:
        """è½¬æ¢å›¾ç‰‡ä¸ºbase64æ ¼å¼"""
        with open(path,"rb") as f:
            data=base64.b64encode(f.read()).decode('utf-8')
            return f"data:{self._get_mime_type(path)};base64,{data}"

    def generate_embeddings(self,paths:List[str],batch_size:Optional[int]=None,lock:Optional[object]=None)->Tuple[List[np.ndarray],List[str]]:
        """æ‰¹é‡è·å–å›¾ç‰‡embeddingsï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        bs=batch_size or self.batch_sz
        embeds,success=[],[]
        # é¢„è¿‡æ»¤ï¼šæ„ŸçŸ¥å“ˆå¸Œå»é‡ï¼Œå‡å°‘APIè°ƒç”¨
        paths=self._prefilter_by_hash(paths)
        for i in range(0,len(paths),bs):
            batch=paths[i:i+bs];data,valid,need=[],[],[]
            self.logger.info(f"Processing batch {i//bs+1}/{(len(paths)+bs-1)//bs}")
            for p in batch:
                if p in self._emb_cache:
                    embeds.append(self._emb_cache[p]);success.append(p);continue
                try:
                    b64=self._to_base64(p)
                    data.append({"content":[{"type":"image_url","image_url":{"url":b64}}]});valid.append(p)
                    need.append(p)
                except Exception as e:
                    self.logger.warning(f"Failed to process {p}: {e}")
            if not data:continue
            try:
                if lock:
                    with lock:
                        emb=self._call_api(data);time.sleep(self.API_DELAY)
                else:
                    emb=self._call_api(data);time.sleep(self.API_DELAY)
                for p,e in zip(need,emb):self._emb_cache[p]=e;embeds.append(e);success.append(p)
            except Exception as e:
                self.logger.warning(f"API call failed: {e}")
        return embeds,success

    def _call_api(self,data:List[Dict[str,Any]])->List[np.ndarray]:
        """è°ƒç”¨Cohere APIè·å–embeddings"""
        try:
            resp=self.client.embed(model=self.embed_model,input_type="image",embedding_types=["float"],inputs=data)
            return [np.array(e) for e in resp.embeddings.float_]
        except Exception as e:
            raise RuntimeError(f"Cohere API error: {e}")

    @staticmethod
    def calc_similarity(e1:np.ndarray,e2:np.ndarray)->float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        n1,n2=np.linalg.norm(e1),np.linalg.norm(e2)
        return 0.0 if n1==0 or n2==0 else float(np.dot(e1,e2)/(n1*n2))

    def remove_duplicates(self,paths:List[str],embeds:List[np.ndarray])->List[str]:
        """åŸºäºembeddingsç›¸ä¼¼åº¦å»é‡ï¼ˆå‘é‡åŒ–/FAISSï¼‰"""
        if len(paths)!=len(embeds):raise ValueError("paths and embeddings length mismatch")
        if not paths:return []
        # å½’ä¸€åŒ–
        E=np.array([e/np.linalg.norm(e) if np.linalg.norm(e)>0 else e for e in embeds],dtype=np.float32)
        if faiss is None:
            keep_idx=[];R=None
            for i,v in enumerate(E):
                if R is not None:
                    sims=R@v
                    if float(sims.max())>self.sim_thresh:continue
                keep_idx.append(i)
                R=v[None,:] if R is None else np.vstack([R,v])
            result=[paths[i] for i in keep_idx]
            self.logger.info(f"Removed {len(paths)-len(result)} duplicates, kept {len(result)}")
            return result
        # ä½¿ç”¨FAISSå†…ç§¯ç´¢å¼•
        d=E.shape[1];index=faiss.IndexFlatIP(d);kept=[];added=False
        for i,v in enumerate(E):
            if added:
                D,_=index.search(v.reshape(1,-1).astype(np.float32),1)
                if float(D[0,0])>self.sim_thresh:continue
            kept.append(i)
            index.add(v.reshape(1,-1));added=True
        result=[paths[i] for i in kept]
        self.logger.info(f"Removed {len(paths)-len(result)} duplicates, kept {len(result)}")
        return result

    def save_unique_frames(self,paths:List[str],output_dir:str,copy_files:bool=True)->List[str]:
        """ä¿å­˜å»é‡åçš„å›¾ç‰‡"""
        os.makedirs(output_dir,exist_ok=True)
        saved=[]
        for i,src in enumerate(paths):
            dst=os.path.join(output_dir,f"unique_frame_{i+1:06d}.jpg")
            try:
                (shutil.copy2 if copy_files else shutil.move)(src,dst)
                saved.append(dst)
            except Exception as e:
                self.logger.warning(f"Failed to save {src}: {e}")
        self.logger.info(f"Saved {len(saved)} frames to {output_dir}")
        return saved

    def process_video_frames(self,video_path:str,start_time:float,end_time:float,output_dir:str,
                           fps:float=1.0,temp_dir:Optional[str]=None,keep_temp:bool=False,
                           lock:Optional[object]=None)->Dict[str,Any]:
        """è§†é¢‘å¸§å»é‡å¤„ç†æµç¨‹"""
        self.logger.info(f"Processing {video_path} [{start_time}s-{end_time}s] fps={fps} thresh={self.sim_thresh}")

        temp_created=temp_dir is None
        if temp_created:temp_dir=tempfile.mkdtemp(prefix="video_dedup_")
        else:os.makedirs(temp_dir,exist_ok=True)

        try:
            self.logger.info("1. Extracting frames...")
            frames=self.extract_frames(video_path,start_time,end_time,fps,temp_dir)
            # é¢„è¿‡æ»¤ï¼Œå‡å°‘åç»­embeddingè°ƒç”¨
            frames=self._prefilter_by_hash(frames)

            self.logger.info("2. Getting embeddings...")
            embeds,success=self.generate_embeddings(frames,lock=lock)
            if len(embeds)!=len(frames):
                self.logger.warning(f"Got {len(embeds)} embeddings for {len(frames)} frames");frames=success

            self.logger.info("3. Removing duplicates...")
            unique=self.remove_duplicates(frames,embeds)

            self.logger.info("4. Saving unique frames...")
            saved=self.save_unique_frames(unique,output_dir)
            # å°†embeddingæŒ‰ä¿å­˜åçš„è·¯å¾„å¯¹é½ï¼Œé¿å…è·¯å¾„å˜æ›´å¯¼è‡´æ— æ³•å¤ç”¨
            ftomap={p:e for p,e in zip(frames,embeds)}
            embed_map={saved[i]:ftomap.get(unique[i]) for i in range(len(saved))}

            result={"video_path":video_path,"time_range":(start_time,end_time),"fps":fps,
                   "total_frames":len(frames),"unique_frames":len(saved),
                   "duplicates_removed":len(frames)-len(saved),"similarity_threshold":self.sim_thresh,
                   "output_dir":output_dir,"saved_paths":saved,"embeddings":embed_map}

            self.logger.info("âœ… Process completed!")
            return result
        finally:
            if temp_created and not keep_temp:
                try:
                    shutil.rmtree(temp_dir)
                    self.logger.info(f"Cleaned temp: {temp_dir}")
                except Exception as e:
                    self.logger.warning(f"Failed cleanup: {e}")

    def _parse_time(self,t:str)->float:
        """æ—¶é—´å­—ç¬¦ä¸²è½¬ç§’æ•°"""
        try:
            parts=t.split(':')
            h,m=int(parts[0]),int(parts[1])
            s_parts=parts[2].split('.')
            s=int(s_parts[0])
            ms=int(s_parts[1]) if len(s_parts)>1 else 0
            return h*3600+m*60+s+ms/1000
        except (ValueError,IndexError) as e:
            raise ValueError(f"æ—¶é—´æ ¼å¼é”™è¯¯: {t}, {e}")

    def extract_segment_frames(self,video_path:str,start_time:str,end_time:str,output_dir:str,
                             text_summary:str="",enable_alignment:bool=True)->List[str]:
        """æå–æ—¶é—´æ®µè§†é¢‘å¸§å¹¶å»é‡ï¼Œæ”¯æŒå›¾æ–‡å¯¹é½"""
        start_sec,end_sec=self._parse_time(start_time),self._parse_time(end_time)
        seg_dir=os.path.join(output_dir,f"segment_{start_time.replace(':','-')}_to_{end_time.replace(':','-')}")
        os.makedirs(seg_dir,exist_ok=True)

        try:
            # 1. å…ˆæå–å’Œå»é‡å¸§ï¼ˆè‡ªé€‚åº”FPSï¼Œç›®æ ‡<=10å¸§ï¼‰
            dur=end_sec-start_sec;seg_fps=self._choose_fps(dur,target=10)
            result=self.process_video_frames(video_path,start_sec,end_sec,seg_dir,seg_fps,keep_temp=False,lock=self._lock)
            frame_paths=result.get("saved_paths",[]);embed_map=result.get("embeddings",{})

            # 2. å¦‚æœå¯ç”¨å¯¹é½ä¸”æœ‰æ–‡æœ¬æ‘˜è¦ï¼Œè¿›è¡Œå›¾æ–‡å¯¹é½ï¼ˆå¤ç”¨embeddingï¼‰
            if enable_alignment and self.enable_text_alignment and text_summary.strip() and frame_paths:
                aligned_paths=self.align_frames_with_text(frame_paths,text_summary,embeds=embed_map)
                # é‡æ–°ä¿å­˜å¯¹é½åçš„å¸§åˆ°æ–°ç›®å½•
                aligned_dir=os.path.join(seg_dir,"aligned")
                final_paths=self.save_unique_frames(aligned_paths,aligned_dir,copy_files=True)
                return final_paths

            return frame_paths
        except Exception as e:
            self.logger.error(f"æ®µ {start_time}-{end_time} å¤„ç†å¤±è´¥: {e}")
            return []

    def _process_segment(self,data:tuple)->dict:
        """å¤„ç†å•ä¸ªæ—¶é—´æ®µ"""
        i,seg,vid_path,frames_dir,out_dir=data
        start,end,summary=seg.get("start_time",""),seg.get("end_time",""),seg.get("summary","")

        try:
            # ä¼ å…¥æ‘˜è¦æ–‡æœ¬è¿›è¡Œå›¾æ–‡å¯¹é½
            paths=self.extract_segment_frames(vid_path,start,end,frames_dir,summary,enable_alignment=True)
            rel_paths=[os.path.relpath(p,out_dir) for p in paths]
        except Exception as e:
            self.logger.error(f"æ®µ {start}-{end} å¤±è´¥: {e}")
            rel_paths=[]

        return {"segment_id":i+1,"start_time":start,"end_time":end,
                "duration_seconds":self._parse_time(end)-self._parse_time(start),
                "summary":summary,"key_frames":rel_paths,"frame_count":len(rel_paths)}

    def align_frames_with_text(self,frame_paths:List[str],text_summary:str,max_frames:int=None,embeds:Optional[Dict[str,np.ndarray]]=None)->List[str]:
        """ä½¿ç”¨Cohereè·¨æ¨¡æ€åµŒå…¥å¯¹é½å›¾æ–‡ï¼ˆå¤ç”¨ç¼“å­˜ï¼‰"""
        if max_frames is None:max_frames=self.max_aligned_frames
        if not frame_paths or not text_summary.strip():return frame_paths[:max_frames]
        try:
            text_input={"content":[{"type":"text","text":text_summary}]}
            text_resp=self.client.embed(model=self.embed_model,input_type="search_query",embedding_types=["float"],inputs=[text_input])
            t=np.array(text_resp.embeddings.float_[0])
            # å‡†å¤‡å›¾åƒåµŒå…¥ï¼Œä¼˜å…ˆä»å‚æ•°/ç¼“å­˜è·å–
            img_vecs=[];valid_paths=[];missing=[]
            for p in frame_paths:
                v=embeds.get(p) if (embeds is not None and p in embeds) else None
                if v is None:v=self._emb_cache.get(p)
                if v is None:missing.append(p)
                else:img_vecs.append(v);valid_paths.append(p)
            if missing:
                new_vecs,_=self.generate_embeddings(missing)
                for p,v in zip(missing,new_vecs):self._emb_cache[p]=v;img_vecs.append(v);valid_paths.append(p)
            if not img_vecs:return frame_paths[:max_frames]
            sims=[self.calc_similarity(t,v) for v in img_vecs]
            idx=sorted(range(len(sims)),key=lambda i:sims[i],reverse=True)
            aligned=[valid_paths[i] for i in idx[:max_frames]]
            self.logger.info(f"æ–‡æœ¬å¯¹é½: {len(frame_paths)}å¸§ -> {len(aligned)}å¸§, æœ€é«˜ç›¸ä¼¼åº¦: {max(sims):.3f}")
            return aligned
        except Exception as e:
            self.logger.warning(f"å›¾æ–‡å¯¹é½å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹å¸§");return frame_paths[:max_frames]

    def generate_multimodal_notes(self,video_path:str,summary_json_path:str,output_dir:str)->str:
        """ç”Ÿæˆå›¾æ–‡æ··æ’ç¬”è®°ï¼ˆå¹¶å‘å¤„ç†ï¼‰"""
        with open(summary_json_path,'r',encoding='utf-8') as f:
            data=json.load(f)

        summaries=data.get("summaries",[])
        if not summaries:raise ValueError("æ‘˜è¦æ•°æ®ä¸ºç©º")

        os.makedirs(output_dir,exist_ok=True)
        frames_dir=os.path.join(output_dir,"frames")
        os.makedirs(frames_dir,exist_ok=True)

        tasks=[(i,seg,video_path,frames_dir,output_dir) for i,seg in enumerate(summaries)]
        notes=[]

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_idx={executor.submit(self._process_segment,task):task[0] for task in tasks}
            results={}

            for future in concurrent.futures.as_completed(future_to_idx):
                idx=future_to_idx[future]
                try:
                    results[idx]=future.result()
                except Exception as e:
                    self.logger.error(f"æ®µ {idx+1} å¤±è´¥: {e}")
                    seg=summaries[idx]
                    results[idx]={"segment_id":idx+1,"start_time":seg.get("start_time",""),
                                 "end_time":seg.get("end_time",""),"duration_seconds":0,
                                 "summary":seg.get("summary",""),"key_frames":[],"frame_count":0}

            for i in range(len(summaries)):
                if i in results:notes.append(results[i])

        final={"video_info":{"source_video":os.path.basename(video_path),"total_segments":len(notes),
                            "generated_at":datetime.now().isoformat(),
                            "processing_mode":f"concurrent (workers={self.max_workers})"},
               "segments":notes,
               "statistics":{"total_frames":sum(n["frame_count"] for n in notes),
                           "segments_with_frames":len([n for n in notes if n["frame_count"]>0])}}

        out_file=os.path.join(output_dir,"multimodal_notes.json")
        with open(out_file,'w',encoding='utf-8') as f:
            json.dump(final,f,ensure_ascii=False,indent=4)
        return out_file

    def export_to_markdown(self,notes_json_path:str,output_path:str=None,image_base_path:str=None,for_web:bool=True)->str:
        """å¯¼å‡ºä¸ºMarkdownæ ¼å¼
        Args:
            for_web: True=ç”Ÿæˆwebè®¿é—®è·¯å¾„(/storage/...)ï¼ŒFalse=ç”Ÿæˆç›¸å¯¹è·¯å¾„(multimodal_notes/...)
        """
        with open(notes_json_path,'r',encoding='utf-8') as f:
            data=json.load(f)

        if not output_path:output_path=f"{Path(notes_json_path).stem}.md"
        if not image_base_path:image_base_path=str(Path(notes_json_path).parent)

        content=self._gen_markdown(data,output_path,image_base_path,for_web=for_web)
        with open(output_path,'w',encoding='utf-8') as f:
            f.write(content)
        return output_path

    def _gen_markdown(self,data:Dict[str,Any],output_path:str=None,img_base:str=None,for_web:bool=True)->str:
        """ç”ŸæˆMarkdownå†…å®¹"""
        info,segs,stats=data.get("video_info",{}),data.get("segments",[]),data.get("statistics",{})
        lines=[]

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        lines.extend([f"# ğŸ“¹ è§†é¢‘ç¬”è®°ï¼š{info.get('source_video','æœªçŸ¥è§†é¢‘')}","",
                     "## ğŸ“Š åŸºæœ¬ä¿¡æ¯","",
                     f"- **è§†é¢‘æ–‡ä»¶**: {info.get('source_video','æœªçŸ¥')}",
                     f"- **ç”Ÿæˆæ—¶é—´**: {info.get('generated_at','æœªçŸ¥')}",
                     f"- **æ€»æ—¶é—´æ®µ**: {info.get('total_segments',0)}",
                     f"- **æ€»å…³é”®å¸§**: {stats.get('total_frames',0)}",
                     f"- **æœ‰æ•ˆæ—¶é—´æ®µ**: {stats.get('segments_with_frames',0)}",""])

        # ç›®å½•
        lines.extend(["## ğŸ“‘ ç›®å½•",""])
        for i,seg in enumerate(segs,1):
            start,end=seg.get("start_time",""),seg.get("end_time","")
            lines.append(f"{i}. [{start} - {end}](#section-{i})")
        lines.extend(["","## ğŸ“ è¯¦ç»†å†…å®¹",""])

        # è¯¦ç»†å†…å®¹
        for i,seg in enumerate(segs,1):
            start,end=seg.get("start_time",""),seg.get("end_time","")
            dur,summary=seg.get("duration_seconds",0),seg.get("summary","")
            frames=seg.get("key_frames",[])

            lines.extend([f"### <a id='section-{i}'></a>æ—¶é—´æ®µ {i}","",f"**â° æ—¶é—´**: {start} - {end} ({dur:.1f}ç§’)","",
                         "**ğŸ“‹ æ‘˜è¦**:","",summary,""])

            if frames:
                lines.extend([f"**ğŸ–¼ï¸ å…³é”®å¸§** ({len(frames)}å¼ ):",""])
                for fp in frames:
                    name=Path(fp).name
                    if for_web:
                        # Webè®¿é—®ï¼šéœ€è¦ç»å¯¹è·¯å¾„ï¼Œä»img_baseæå–task_id
                        task_id = Path(img_base).name if img_base else "unknown"
                        path=f"/storage/tasks/{task_id}/multimodal_notes/{fp}"
                    else:
                        # PDFå¯¼å‡ºï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„
                        path=f"multimodal_notes/{fp}"
                    lines.append(f"![{name}]({path})")
                lines.append("")
            else:
                lines.extend(["*è¯¥æ—¶é—´æ®µæ— å…³é”®å¸§*",""])
            lines.extend(["---",""])

        # é¡µè„š
        lines.extend(["## ğŸ”§ ç”Ÿæˆä¿¡æ¯","","æœ¬ç¬”è®°ç”±è§†é¢‘å¤„ç† API è‡ªåŠ¨ç”Ÿæˆ",
                     f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"])
        if output_path:lines.append(f"è¾“å‡ºæ–‡ä»¶: {output_path}")

        return "\n".join(lines)


def create_multimodal_service(cohere_api_key:str,enable_text_alignment:bool=True,**kwargs)->MultimodalService:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºå¤šæ¨¡æ€æœåŠ¡"""
    return MultimodalService(cohere_api_key,enable_text_alignment=enable_text_alignment,**kwargs)
